{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>CRTS Light Curve Classification</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Getting the data</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/gettingthedata1.png\"/> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>url_extraction.py</h3>\n",
    "<h5>Code to get a list of all URLs that need to be accessed in order to get the data</h5>\n",
    "\n",
    "<p><b>URLHTMLParser:</b> parser to detect when there's a link to a light curve</p>\n",
    "<p><b>save:</b> code to save the/blist of light curve's URls to a file</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/gettingthedata2.png\"/> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from html.parser import HTMLParser\n",
    "import csv\n",
    "\n",
    "url_listlc = []\n",
    "\n",
    "class URLHTMLParser(HTMLParser):\n",
    "    global url_list\n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        lc = \"Lightcurve\"\n",
    "        if tag == 'a' and len(attrs) >= 1 and len(attrs[0]) >=2:\n",
    "            if attrs[0][1]!= None and lc in attrs[0][1]:\n",
    "                url_attr = attrs[2][1]\n",
    "                split_attr = url_attr.split(\"'\")\n",
    "                url = split_attr[1]\n",
    "                url = url[0:len(url)-1]\n",
    "                url_list.append(url)\n",
    "\n",
    "def save(url_list, file_name):\n",
    "    with open(file_name, 'w') as urlFile:\n",
    "        wr = csv.writer(urlFile, lineterminator='\\n')\n",
    "        for url in url_list:\n",
    "            wr.writerow([url])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Iteration over 3 Survey's URLs to get all light curves later<p>\n",
    "<p>List of urls is saved to <b>lc_urls.csv</b><p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    urls = [\"http://nesssi.cacr.caltech.edu/catalina/Allns.arch.html#table1\",\n",
    "            \"http://nesssi.cacr.caltech.edu/MLS/Allns.arch.html\",\n",
    "            \"http://nesssi.cacr.caltech.edu/SSS/Allns.html\"]\n",
    "    for index, url in enumerate(urls):\n",
    "        url_list = []\n",
    "        html = urlopen(url)\n",
    "        the_page = str(html.read())\n",
    "        parser = URLHTMLParser()\n",
    "        parser.feed(the_page)\n",
    "        file_name = \"data\"+str(index)+\"/lc_urls.csv\"\n",
    "        save(url_list, file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>lc_extraction.py</h3>\n",
    "<h5>Code to get a light curves from URLs and save them to files.</h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/gettingthedata3.png\"/> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Same logic as before, only now the iteration is over the URLs saved before</>\n",
    "\n",
    "<p><b>LCHTMLParser:</b> parser to detect where the light curve data is and parse it</p>\n",
    "<p><b>save:</b> code to save the light curves into different files each</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lc_list = []\n",
    "lc_point_list=[]\n",
    "\n",
    "class LCHTMLParser(HTMLParser):\n",
    "    global lc_point_list\n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        if tag == 'area':\n",
    "            #get coordinates\n",
    "            coords = attrs[1][1] \n",
    "            #get date, mag, error\n",
    "            point = attrs[2][1] \n",
    "            xye = point.split(\";\")\n",
    "            clean_point=[]\n",
    "            for i in xye:\n",
    "                separate = i.split(\"'\")\n",
    "                if len(separate) > 1:\n",
    "                    item = separate[1] \n",
    "                    l = len(item)\n",
    "                    clean_item = item[0:l-1]\n",
    "                    clean_point.append(clean_item)\n",
    "            date = clean_point[0]\n",
    "            mag = clean_point[1]\n",
    "            error = clean_point[2]\n",
    "            lc_point_list.append([coords, date, mag, error])\n",
    "\n",
    "def get_page_content(url): \n",
    "    html = urlopen(url)\n",
    "    the_page = str(html.read())\n",
    "    parser = LCHTMLParser()\n",
    "    parser.feed(the_page)\n",
    "\n",
    "def save_lc(url,lc_point_list, index):\n",
    "    name = url.split(\"/\")\n",
    "    name = \"data\"+str(index)+\"/\"+name[len(name)-1].split(\"p\")[0]\n",
    "    print(\"Saving lc to file \", name)\n",
    "    fieldnames = [\"coords\",\"date\",\"mag\",\"error\"]\n",
    "    print(name)\n",
    "    with open(name, 'w') as lcFile:\n",
    "        writer = csv.writer(lcFile)\n",
    "        writer.writerow(fieldnames)\n",
    "        writer.writerows(lc_point_list)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    for index in range(3):\n",
    "        with open(\"data\"+str(index)+\"/lc_urls.csv\", newline='\\n') as urlFile:\n",
    "            reader = csv.reader(urlFile)\n",
    "            for row in reader:\n",
    "                url = row[0]\n",
    "                print(\"Getting lc from \", url)\n",
    "                get_page_content(url)\n",
    "                save_lc(url,lc_point_list, index)\n",
    "                lc_point_list=[]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>meta_data_extraction.py</h3>\n",
    "<h5>Code to get light curve metadata, in case it's needed later.</h5>\n",
    "<p>Same logic as before</p>\n",
    "<p><b>MetadataHTMLParser:</b> parser to detect when there's a new object and parse its attributes</p>\n",
    "<p><b>save:</b> code to produce a metada file: <b>metadata_lc.csv</b> </p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = []\n",
    "data_list_item = []\n",
    "\n",
    "class metaDataHTMLParser(HTMLParser):\n",
    "\n",
    "    def handle_endtag(self, tag):\n",
    "        global data_list\n",
    "        global data_list_item\n",
    "        if tag == 'tr':\n",
    "            data_list.append(data_list_item)\n",
    "            data_list_item = []\n",
    "\n",
    "    def handle_data(self, data):\n",
    "        global data_list_item\n",
    "        data_list_item.append(data)\n",
    "\n",
    "def save_meta_data(data_list,index):\n",
    "    with open(\"data\"+str(index)+\"/lc_metadata.csv\", 'w') as metaDataFile:\n",
    "        wr = csv.writer(metaDataFile)\n",
    "        fieldnames = ['CRTS ID', 'RA (J2000)', 'Dec (J2000)', 'UT Date', 'Mag', 'CSS images', 'SDSS', 'Others', 'Followed', 'Last', 'LC', 'FC', 'Classification','SubClassification']\n",
    "        wr.writerow(fieldnames)\n",
    "        wr.writerows(data_list)\n",
    "   \n",
    "if __name__ == \"__main__\":\n",
    "    urls = [\"http://nesssi.cacr.caltech.edu/catalina/Allns.arch.html#table1\",\n",
    "            \"http://nesssi.cacr.caltech.edu/MLS/Allns.arch.html\",\n",
    "            \"http://nesssi.cacr.caltech.edu/SSS/Allns.html\"]\n",
    "    for index, url in enumerate(urls):\n",
    "        data_list = []\n",
    "        html = urlopen(url)\n",
    "        the_page = str(html.read())\n",
    "        parser = metaDataHTMLParser()\n",
    "        parser.feed(the_page)\n",
    "        data_list = data_list[1:]\n",
    "        for item in data_list:\n",
    "            item.pop(0)\n",
    "            id = item.pop(0)\n",
    "            id = id[:len(id)-2]\n",
    "            item.insert(0,id)\n",
    "            item.pop(12)\n",
    "        save_meta_data(data_list, index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Data Pre Processing</h2>\n",
    "<h3>data_pre_processing.py</h3>\n",
    "<h5>Code to decide which classes are considered and add tag each object with a class.</h5>\n",
    "<p><b>load_data:</b> Data is loaded as pandas data frame and all duplicates are dropped. We use the <b>metadata_lc.csv</b> file for this.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def load_data():\n",
    "    fieldnames = ['ID', 'RA (J2000)', 'Dec (J2000)', 'UT Date', 'Mag', 'images', 'SDSS', 'Others', 'Followed', 'Last', 'LC', 'FC', 'Classification','SubClassification']\n",
    "    data0 = pd.read_csv(\"data0/lc_metadata.csv\", sep=\",\", names=fieldnames, skiprows=1)\n",
    "    data0 = data0.drop_duplicates()\n",
    "    data0[\"Survey\"] = \"CSS\"\n",
    "    data1 = pd.read_csv(\"data1/lc_metadata.csv\", sep=\",\", names=fieldnames, skiprows=1)\n",
    "    data1[\"Survey\"] = \"MLS\"\n",
    "    data1 = data1.drop_duplicates()\n",
    "    data2 = pd.read_csv(\"data2/lc_metadata.csv\", sep=\",\", names=fieldnames, skiprows=1)\n",
    "    data2[\"Survey\"] = \"SSS\"\n",
    "    data2 = data2.drop_duplicates()\n",
    "    frames = [data0, data1, data2]\n",
    "    data = pd.concat(frames, ignore_index=True)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><b>add_tags:</b> The idea is to add an extra column to the data frame that contains the class to which it belongs</p>\n",
    "<p>First, we count how many classes there are.</p>\n",
    "<p>We get a list of classes we'll use. Only classes that have more than 100 objects are considered. This number is arbitrary.</p>\n",
    "<p>We filter the list of 'main classes' and get rid of all the classes that have a question mark in them</p>\n",
    "<p>We add a tag to each object. If the classification the object already has is in the main classes list, then that's the tag. If not, the object gets tagged as \"other\"</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_tags(data):\n",
    "    classes = data.groupby('Classification').count()    \n",
    "    main_classes = classes[classes[\"ID\"] > 100]\n",
    "    tags = list(filter(lambda tag: not \"?\" in tag, list(main_classes.index)))\n",
    "    data['tag'] = np.where(\n",
    "        data['Classification'].isin(tags), data['Classification'], 'Other'\n",
    "    )\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now correctly tagged some classes (main ones), but we could try and tag a few more. Two decisions were made:\n",
    "<p>1.- All objects classified as 2 things (ex: CV/SN) will be considered to be of the 1st class.</p>\n",
    "<p>2.- All objects classified with question mark (ex: SN?) will be considered to be of that class.</p>\n",
    "\n",
    "<p><b>consider_1st_class:</b>decision number 1</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def consider_1st_class(data):\n",
    "    # consider all classes containing \"/\" as 1st class \n",
    "    data['tag'] = np.where(\n",
    "        (data['Classification'].str.contains(\"/\")) &\n",
    "        (data['Classification'].str.split(\"/\").str[0].isin(tags)), \n",
    "        data['Classification'].str.split(\"/\").str[0], data[\"tag\"]\n",
    "    )\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><b>ignore_question_marks:</b> decision number 2</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ignore_question_marks(data):\n",
    "     # consider all classes containing \"?\" as if they didn't\n",
    "    data['tag'] = np.where(\n",
    "        (data['Classification'].str.contains('\\\\?',na=False)) &\n",
    "        (~ data['Classification'].str.contains('/', na=False))&\n",
    "        (data['Classification'].str.split(\"?\").str[0].isin(tags)), \n",
    "        data['Classification'].str.split(\"?\").str[0], data[\"tag\"]\n",
    "    )\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><b>tags_to_numbers:</b>We give all tags a numerical code</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tags_to_numbers(data):\n",
    "    tags.append(\"Other\")\n",
    "    data['tag'] = data['tag'].map(lambda tag: tags.index(tag),\n",
    "               na_action=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <p>All the functions above are run on main method and save the final data frame to <b>tagged_meta_data.csv</b></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    data = load_data()\n",
    "    data = add_tags(data)\n",
    "    data = consider_1st_class(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Getting the features</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Now that we have a tagged dataset, how do we get features?</p>\n",
    "<h3>FATS library</h3>\n",
    "<h4>Feature Analisis for Time Series, by Isadora Nun</h4>\n",
    "<b>https://github.com/isadoranun/FATS</b>\n",
    "\n",
    "<p><b>PROBLEM! </b> Library incompatible with what we have so far:</p>\n",
    "<p>-Uses python3 instead of python2.7</p>\n",
    "<p>-Lots of deprecated dependencies</p>\n",
    "<p>-Some functions return NaN or Infinite values</p>\n",
    "\n",
    "<p><b>LONG TERM SOLUTION (TO DO): </b> Do pull request and make library compatible with python3. Replace deprecated dependencies. This will take some time, but it's definitely worth doing.</p>\n",
    "<p><b>TEMPORARY SOLUTION: </b> Go into the library source code, get the functions needed, create file with them and use them. The file produced is called <b>feature_functions.py</b></p>\n",
    "<p>Example of feture:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def amplitude(data):\n",
    "    magnitude = data[0]\n",
    "    N = len(magnitude)\n",
    "    sorted_mag = np.sort(magnitude)\n",
    "    return (np.median(sorted_mag[-int(math.ceil(0.05 * N)):]) -\n",
    "            np.median(sorted_mag[0:int(math.ceil(0.05 * N))])) / 2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Which features?</h3>\n",
    "<p>In FATS library (and all over the internet) there are many features that can be calculated for Time Series. To decide which ones to calculate, we use two criteria:</p>\n",
    "<p>1.-The feature need at most magnitude, time and error to be calculated</p>\n",
    "<p>2.-The feature doesn't need a long time to be calculated. We define \"long\" as [ms] and we keep all features that can be calculated in [µs]. To get a sense of how long it will take to calculate a feature, we calculate all of them for just one light curve and measure time.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Amplitude  \n",
    "Rcs  \n",
    "StetsonK  \n",
    "Meanvariance  \n",
    "Autocor_length  \n",
    "Con  \n",
    "Beyond1Std  \n",
    "SmallKurtosis  \n",
    "Std  \n",
    "Skew  \n",
    "MaxSlope  \n",
    "MedianAbsDev  \n",
    "MedianBRP  \n",
    "PairSlopeTrend  \n",
    "FluxPercentileRatioMid20  \n",
    "FluxPercentileRatioMid35  \n",
    "FluxPercentileRatioMid50  \n",
    "FluxPercentileRatioMid65  \n",
    "FluxPercentileRatioMid80  \n",
    "PercentDifferenceFluxPercentile  \n",
    "PercentAmplitude  \n",
    "LinearTrend  \n",
    "Eta_e  \n",
    "Mean  \n",
    "Q31  \n",
    "AndersonDarling  \n",
    "Gskew  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Feature extraction</h3>\n",
    "<h3>feature_extraction.py</h3>\n",
    "\n",
    "<p><b>function_list:</b> get list of features and turn them into list of functions</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import feature_functions as f\n",
    "from utils import feature_list\n",
    "\n",
    "function_list = [getattr(f, x) for x in feature_list]\n",
    "featuresdf = pd.DataFrame(columns=feature_list)\n",
    "errorsdf = pd.DataFrame(columns=['filename','error'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><b>get_features:</b> Load light curve and map function list to it</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(filename,tag, objid):\n",
    "    global featuresdf\n",
    "    data = pd.read_csv(filename, sep=\",\", names=[\"coords\", \"time\", \"mag\", \"error\"], skiprows=1)\n",
    "    mag = data['mag']\n",
    "    time = pd.to_numeric(data[\"time\"].str.split(\"(\").str[0])\n",
    "    error = data[\"error\"]\n",
    "    lc = np.array([np.asarray(mag.tolist()),np.asarray(time.tolist()),np.asarray(error.tolist())])\n",
    "    features = list(map(lambda f: f(lc), function_list))\n",
    "    row = pd.DataFrame([features],columns=feature_list)\n",
    "    row[\"id\"] = objid\n",
    "    row['tag'] = tag\n",
    "    featuresdf = pd.concat([featuresdf, row],  ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>On main method: do the above for all light curves and store the data in <b>tagged_features.csv</b></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    fieldnames =  [\"ID\",\"RA (J2000)\",\"Dec (J2000)\",\"UT Date\",\"Mag\",\"images\",\"SDSS\",\n",
    "        \"Others\",\"Followed\",\"Last\",\"LC\",\"FC\",\"Classification\",\"SubClassification\",\"Survey\",\"tag\"]\n",
    "    tagged_metadata = pd.read_csv(\"data/tagged_meta_data.csv\", sep=\",\", names=fieldnames, skiprows=1)\n",
    "    for index, row in tagged_metadata.iterrows():\n",
    "        filename = \"\"\n",
    "        tag = row['tag']\n",
    "        objid =\"\"\n",
    "        if row['Survey'] == 'CSS':\n",
    "            objid = 'CSS'+str(row['images'])\n",
    "            filename = \"data0/\"+str(row['images'])\n",
    "        elif row['Survey'] == 'MLS':\n",
    "            objid = 'MLS'+str(row['images'])\n",
    "            filename = \"data1/\"+str(row['images'])\n",
    "        elif row['Survey'] == 'SSS':\n",
    "            objid = 'SSS'+str(row['images'])\n",
    "            filename = \"data2/\"+str(row['images'])\n",
    "        try:\n",
    "            get_features(filename, tag, objid)\n",
    "        except Exception as e:\n",
    "            error = {'filename':[filename],'error': [str(e)]}\n",
    "            error_row = pd.DataFrame(data=error)\n",
    "            errorsdf = pd.concat([errorsdf, error_row],ignore_index=True)\n",
    "    \n",
    "    featuresdf.to_csv(\"data/tagged_features.csv\", sep=',')\n",
    "    errorsdf.to_csv(\"data/errors_processing_features.csv\", sep=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Feature Pre Processing</h2>\n",
    "<h3>feature_pre_processing.py</h3>\n",
    "<h5>Code to clean features and make sure they don't make the classification models crush</h5>\n",
    "<p><b>remove_nan_inf:</b> Features are handled as pandas data frame. To remove both infinite and nan values, we replace all infinites by nans and handle them equally. We find out which features give NaN values and or wich objects. If a feature gives NaN value for more than 10 objects, the feature is dropped. If not, the object is dropped. </p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import feature_list\n",
    "\n",
    "def remove_nan_inf(X, tagged_features):\n",
    "    X=X.replace([np.inf, -np.inf], np.nan)\n",
    "    where_are_nans = X.isnull()\n",
    "    sum_nans = where_are_nans.sum()\n",
    "    feature_nans = list(sum_nans[sum_nans[feature_list]>0].keys())\n",
    "    arbitrary = 10\n",
    "    for fnan in feature_nans:\n",
    "        ids_nan = list(where_are_nans[where_are_nans[fnan]==True].index)      \n",
    "        try:\n",
    "            if len(ids_nan) > arbitrary:\n",
    "                tagged_features = tagged_features.drop([fnan],axis=1)\n",
    "            else:\n",
    "                tagged_features = tagged_features.drop(ids_nan)\n",
    "        except Exception as e:\n",
    "            print('error: ',str(e))\n",
    "            print('probably trying to drop value that has already been dropped')\n",
    "    \n",
    "    return tagged_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><b>remove_unknown_others:</b> All objects that belong to 'Unknown' or 'Other' class are removed. This is a first approach, to make classification easier. They should eventually be included</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_unkown_others(tagged_features):    \n",
    "    tagged_features = tagged_features[(tagged_features['tag']!=6) & (tagged_features['tag']!=8)]\n",
    "    print(tagged_features.shape)\n",
    "    return tagged_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>On the main method, we run the above and save the results to <b>clean_tagged_features.csv</b></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    filename = \"data/tagged_features.csv\"\n",
    "    tagged_features = pd.read_csv(filename, sep=\",\")\n",
    "    X = tagged_features[feature_list]\n",
    "    tagged_features = remove_nan_inf(X, tagged_features)\n",
    "    tagged_features = remove_unknown_other(tagged_features)\n",
    "    tagged_features.to_csv(\"data/clean_tagged_features\", sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Classification</h2>\n",
    "<h3>Scikit-learn</h3>\n",
    "<p>As first attempt, we start by using the python scikit-learn library. It's a well known robust library that appears on any search for ML libraries.</p>\n",
    "<h3>sklearn_models.py</h3>\n",
    "<h5>Code to classify light curves into classes and check how it goes.</h5> \n",
    "<p>To start, we use common classificators with default values</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import feature_list\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(),\n",
    "    SVC(),\n",
    "    GaussianProcessClassifier(),\n",
    "    DecisionTreeClassifier(),\n",
    "    RandomForestClassifier(),\n",
    "    MLPClassifier(),\n",
    "    AdaBoostClassifier(),\n",
    "    GaussianNB(),\n",
    "    QuadraticDiscriminantAnalysis()]\n",
    "\n",
    "classifier_names = [\"Nearest Neighbors\", \"RBF SVM\", \"Gaussian Process\",\n",
    "         \"Decision Tree\", \"Random Forest\", \"Neural Net\", \"AdaBoost\",\n",
    "         \"Naive Bayes\", \"QDA\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><b>load_data:</b> we load the clean features and divide the tags from the features</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    filename = \"data/clean_tagged_features.csv\"\n",
    "    data = pd.read_csv(filename, sep=\",\")\n",
    "    X = data[feature_list]\n",
    "    Y = data[\"tag\"] \n",
    "    return X,Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><b>split_datasets:</b> function that receives data set and splits it into trainning data set and test data set<>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_datasets(X,Y):\n",
    "    Xnp = X.as_matrix()\n",
    "    Ynp = Y.as_matrix()\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(Xnp, Ynp, test_size=0.3)\n",
    "    return [X_train, X_test, Y_train, Y_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><b>train_predic:</b> function that receives trainning and testing data set, trains a classificator and predictcs results </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_predict(sets, model):\n",
    "    Xtrain, Xtest, Ytrain, Ytest = sets\n",
    "    trainningT0 = time.time()\n",
    "    model.fit(Xtrain, Ytrain)\n",
    "    trainningT1 = time.time()\n",
    "    predictingT0 = time.time()\n",
    "    predicted = model.predict(Xtest)\n",
    "    predictingT1 = time.time()\n",
    "    dtrainning = trainningT1-trainningT0\n",
    "    dpredicting = predictingT1-predictingT0\n",
    "    return model, predicted, dtrainning, dpredicting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><b>get_model_score:</b> function that cross validates the model and writes the results on a file</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_score(model,predicted, sets, filename, dtrainning, dpredicting):\n",
    "    Xtrain, Xtest, Ytrain, Ytest = sets\n",
    "    validatingT0 = time.time()\n",
    "    cv_scores = cross_val_score(model, Xtrain, Ytrain, cv=5)\n",
    "    validatingT1 = time.time()\n",
    "    dvalidate = validatingT1-validatingT0\n",
    "    with open(filename,\"w\") as file:\n",
    "        report = metrics.classification_report(Ytest, predicted)\n",
    "        mean_score = np.mean(cv_scores)\n",
    "        std = np.std(cv_scores)\n",
    "        score = model.score(Xtest, Ytest)\n",
    "        file.write(\"report: \")\n",
    "        file.write(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>In the main method, we use all of the above. We load the data, split it into trainning and testing data sets and iterate over classifiers to classify the data sets. After that, we cross validate and save the results to a file</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"loading data\")\n",
    "    X, Y = load_data()\n",
    "    print(\"splitting data into trainning and test data sets\")\n",
    "    sets = split_datasets(X,Y)\n",
    "    print(\"trainning models\")\n",
    "    for i, classifier in enumerate(classifiers):\n",
    "        print(\"trainning-testing model \",i,\"/8 :\",classifier_names[i])\n",
    "        model, predicted, tt, tp = train_predict(sets, classifier)\n",
    "        print(\"getting score for model \",i,\"/8 :\",classifier_names[i])\n",
    "        get_model_score(model, predicted, sets, \"results/fl/\"+classifier_names[i]+\".txt\", tt, tp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Results so far</h2>\n",
    "<p>Using all the above features, the classifier that performed better on average was Random Forest, although it did not perform so well either. It's mean score was 72% of precision.</p>\n",
    "<img src=\"img/scores.png\"/>\n",
    "<p>Gaussian process was the second best, but took far to much time compared to the other classifiers, so it's not worth it.</p>\n",
    "<img src=\"img/time.png\"/>\n",
    "<p>If we look in detail at the file for the classifier that performed best, we can see that some classes were better classifed than others. This pattern is repeated through all classifiers. Surprisingly, the class that gets classified better is not the one with most samples.</p><br></br>\n",
    "\n",
    "\n",
    "<div class=\"pull-left\">\n",
    "\n",
    "<table>\n",
    "    \n",
    "<tr><td>Class</td><td>Precision</td><td>Recall</td><td>Support</td></tr>\n",
    "<tr><td>0-AGN</td><td>0.75</td><td>0.86</td><td>975</td></tr>\n",
    "<tr style=\"color:red;\"><td>1-Blazar</td><td>0.35</td><td>0.24</td><td>94</td></tr>\n",
    "<tr><td>2-CV</td><td>0.62</td><td>0.58</td><td>381</td></tr>\n",
    "<tr><td>3-Flare</td><td>0.62</td><td>0.93</td><td>92</td></tr>\n",
    "<tr style=\"color:blue;\"><td>4-HPM</td><td>0.93</td><td>0.94</td><td>236</td></tr>\n",
    "<tr><td>5-SN</td><td>0.72</td><td>0.72</td><td>1048</td></tr>\n",
    "<tr style=\"color:red;\"><td>7-Var</td><td>0.55</td><td>0.11</td><td>55</td></tr>\n",
    "</table>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Before trying out any more libraries, it makes sense to try and figure out which features are best for classifying this type of objects. In hopes of doing that, pairwise feature plots were produced, to see if any particular pair of features makes the division more obvious.</p>\n",
    "<img src=\"img/pp1.png\"/> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
